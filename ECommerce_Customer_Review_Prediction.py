# -*- coding: utf-8 -*-
"""datapreparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CH41Ky36zqM82CTBgImkLsGe1YIH9e9P

# Import folder
This is a dataset from brazilian-ecommerce, the folder_path is named as '/content/brazilian-ecommerce', for runtime we could creat a temperatry folder in colab called 'brazilian-ecommerce' and upload all the original CSVs here.

The file names are all wrapped into dataframes incase for any naming convention，for loop is used to include every files from the folder，caution：do not include any irrelavant files in this folder other than pure dataset.
"""

import pandas as pd
import numpy as np
import os as os
from tabulate import tabulate
import matplotlib.pyplot as plt

folder_path = '/content/brazilian-ecommerce'

# for file_name in os.listdir(folder_path):
#   if file_name.endswith('.csv'):
#      file_path = os.path.join(folder_path, file_name)

dataframes = {}

# Loop through each file in the folder
for file_name in os.listdir(folder_path):
    # Check if the current object is a file and ends with '.csv'
    if file_name.endswith('.csv'):
        # Remove the '.csv' extension
        file_name_cleaned = os.path.splitext(file_name)[0]

        # Remove 'olist_' prefix and '_dataset' suffix if present
        file_name_cleaned = file_name_cleaned.removeprefix('olist_').removesuffix('_dataset')

        # Construct the full file path
        file_path = os.path.join(folder_path, file_name)


        # Read the CSV file into a DataFrame
        dataframes[file_name_cleaned] = pd.read_csv(file_path)


# Example: Accessing a DataFrame
print(dataframes.keys())  # Print all the keys (cleaned file names)
print(dataframes['orders'])  # Access the DataFrame using the cleaned file name


# order_items_df = pd.read_csv('/content/sample_data/olist_order_items_dataset.csv')
# product_df = pd.read_csv('/content/sample_data/olist_products_dataset.csv')
# customers_df = pd.read_csv('/content/sample_data/olist_customers_dataset.csv')
# orders_df = pd.read_csv('/content/sample_data/olist_orders_dataset.csv')
# payments_df = pd.read_csv('/content/sample_data/olist_products_dataset.csv')
# sellers_df = pd.read_csv('/content/sample_data/olist_sellers_dataset.csv')
# geo_df = pd.read_csv('/content/sample_data/olist_geolocation_dataset.csv')
# product_category_name_translation_df = pd.read_csv('/content/sample_data/product_category_name_translation.csv')

#print(order_items_df.head())
# print(product_df.head())
# print(customers_df.head())
# print(orders_df.head())
# print(payments_df.head())
# print(sellers_df.head())
# print(geo_df.head())

# print (order_items_df.info())
# print (product_df.info())
# print (customers_df.info())
# print (orders_df.info())
# print (payments_df.info())
# print (sellers_df.info())
# print (geo_df.info())

# na_order_items= order_items_df.isna().sum()
# na_product= product_df.isna().sum()
# na_customers= customers_df.isna().sum()
# na_orders= orders_df.isna().sum()
# na_payments= payments_df.isna().sum()
# na_sellers= sellers_df.isna().sum()
# na_geo= geo_df.isna().sum()

# print(na_order_items)
# print(na_product)
# print(na_customers)
# print(na_orders)
# print(na_payments)
# print(na_sellers)
# print(na_geo)

"""# View Datasets

For better understanding of each files and easir to lookup for variables for later use, a table is created to be able to show all the column names within each file.
"""

# Create a list to store column information
columns_info = []

# Collect column names for each DataFrame
# for name, df in dataframes.items():
#     columns_info.append({
#         "File Name": name,
#         "Columns": df.columns.tolist()
#     })

# Collect column names for each DataFrame
for name, df in dataframes.items():
    columns_info.append({
        "File Name": name,
        "Columns": ", ".join(df.columns)  # Join column names for better readability
    })

# Convert the list to a DataFrame for a tabular view
#columns_df = pd.DataFrame(columns_info)

# Display the table
#print(columns_df)

# Convert the list to a DataFrame for a tabular view
columns_df = pd.DataFrame(columns_info)

# Print the table using tabulate for a more readable format
print(tabulate(columns_df, headers='keys', tablefmt='fancy_grid'))

#Check for dataframes
print(dataframes['order_items'].head())

"""# Merge usefull datasets

for each datasets that are going to use in the following stage is being extracted to put into a new CSV that contians all initial and sorted datasets.
"""

# Extract necessary DataFrames
sellers = dataframes['sellers'][['seller_id', 'seller_city', 'seller_state','seller_zip_code_prefix']]
customers = dataframes['customers'][['customer_id', 'customer_city', 'customer_state']]
orders = dataframes['orders'][['order_id', 'customer_id', 'order_purchase_timestamp', 'order_delivered_customer_date', 'order_approved_at','order_estimated_delivery_date' ]]
reviews = dataframes['order_reviews'][['order_id', 'review_score']]
products = dataframes['products'][['product_id', 'product_category_name','product_description_lenght','product_photos_qty']]
order_items = dataframes['order_items'][['order_id', 'product_id', 'seller_id','price', 'freight_value']]
product_translation = dataframes['product_category_name_translation'][['product_category_name', 'product_category_name_english']]
geolocation = dataframes['geolocation'][['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state']]
order_payments = dataframes['order_payments'][['order_id', 'payment_type', 'payment_installments', 'payment_value']]

# Merge the DataFrames
# Merge orders with customers
merged_df = pd.merge(orders, customers, on='customer_id')

# result with order_items
merged_df = pd.merge(merged_df, order_items, on='order_id')

# result with sellers
merged_df = pd.merge(merged_df, sellers, on='seller_id')

#  result with products
merged_df = pd.merge(merged_df, products, on='product_id')

# result with reviews
merged_df = pd.merge(merged_df, reviews, on='order_id')

merged_df = pd.merge(merged_df, order_payments, on='order_id')

merged_df = pd.merge(merged_df, product_translation, on='product_category_name')

#merged_df = pd.merge(merged_df, geolocation, left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix')

# Select only the necessary columns
#Include 'price' and 'freight_value' in the list of columns
final_df = merged_df[[
    'payment_installments', 'seller_id','order_delivered_customer_date','product_description_lenght','product_photos_qty',
    'price', 'freight_value','order_approved_at', 'order_estimated_delivery_date', 'customer_state', 'customer_id',
    'review_score', 'product_category_name_english'
]]

# Save the final DataFrame to a new CSV file
final_df.to_csv(os.path.join(folder_path, 'merged_data.csv'), index=False)

print(final_df.head())

# dropna rows from this new final data file
final_df = final_df.dropna()
na_final= final_df.isna().sum()
#print if there is na remains
print(na_final)

#to see how many rows are there
print("Number of rows:", len(final_df))

"""# Exploratory Data Analysis (EDA)"""

plt.bar(final_df['price'], final_df['review_score']) # create a bar chart of the two values
# note we use a "\" symbol so that the apostrophy doesn't close the string
plt.xlabel('Parent\'s Education Level') # x-axis is parent's education level
plt.xticks(rotation=90) # rotate x labels 90 degrees (vertical)
plt.ylabel('Math Score') # y-axis is maths score
plt.title('Parent\'s Education vs Math Score') # add title
plt.plot() # plot the chart to screen

# # Extract necessary DataFrames
# sellers = dataframes['sellers'][['seller_id', 'seller_city', 'seller_state','seller_zip_code_prefix']]
# customers = dataframes['customers'][['customer_id', 'customer_city', 'customer_state']]
# orders = dataframes['orders'][['order_id', 'customer_id', 'order_purchase_timestamp', 'order_delivered_customer_date', 'order_approved_at','order_estimated_delivery_date' ]]
# reviews = dataframes['order_reviews'][['order_id', 'review_score']]
# products = dataframes['products'][['product_id', 'product_category_name']]
# order_items = dataframes['order_items'][['order_id', 'product_id', 'seller_id','price', 'freight_value']]
# product_translation = dataframes['product_category_name_translation'][['product_category_name', 'product_category_name_english']]
# geolocation = dataframes['geolocation'][['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state']]
# order_payments = dataframes['order_payments'][['order_id', 'payment_type', 'payment_installments', 'payment_value']]

# # Merge the DataFrames
# # Merge orders with customers
# merged_df = pd.merge(orders, customers, on='customer_id')

# # result with order_items
# merged_df = pd.merge(merged_df, order_items, on='order_id')

# # result with sellers
# merged_df = pd.merge(merged_df, sellers, on='seller_id')

# #  result with products
# merged_df = pd.merge(merged_df, products, on='product_id')

# # result with reviews
# merged_df = pd.merge(merged_df, reviews, on='order_id')

# merged_df = pd.merge(merged_df, order_payments, on='order_id')

# merged_df = pd.merge(merged_df, product_translation, on='product_category_name')

# #merged_df = pd.merge(merged_df, geolocation, left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix')

# # Select only the necessary columns
# final_df = merged_df[[
#     'payment_installments', 'seller_id','order_delivered_customer_date',
#     'price', 'freight_value','order_approved_at', 'order_estimated_delivery_date',
#     'review_score', 'product_category_name_english'
# ]]

# # Save the final DataFrame to a new CSV file
# final_df.to_csv(os.path.join(folder_path, 'merged_data.csv'), index=False)

# print(final_df.head())

"""# Check for data Cleaning

After the final_df is created, it is important to check the data structure are matching with the desired list, also it needs to be double check for any duplicates and Nas.
"""

# dropna rows from this new final data file
final_df = final_df.dropna()
na_final= final_df.isna().sum()
#print if there is na remains
print(na_final)

#to see how many rows are there
print("Number of rows:", len(final_df))

#Handle Duplicates

# to print numbers of duplicates
print(df.duplicated().sum())

final_df = final_df.drop_duplicates()

#No duplicates here so no need for any more processs

"""# Transforms and concatenation

"""

# product_category_names = 'product_category_name_english'
# catagory_names = final_df[product_category_names].unique()

# #print all product category names
# print(catagory_names)

#create new column for time for delivery(assigned data subtract by delivered data
final_df['delivery_time'] = pd.to_datetime(final_df['order_delivered_customer_date']) - pd.to_datetime(final_df['order_approved_at'])
#convert delivery time to days
final_df['delivery_time'] = final_df['delivery_time'].dt.days



#create new column for time for difference between estimated and actual delivered.
final_df['early_late_delivery'] = pd.to_datetime(final_df['order_estimated_delivery_date']) - pd.to_datetime(final_df['order_delivered_customer_date'])
final_df['early_late_delivery'] = final_df['early_late_delivery'].dt.days



final_df['total_price'] = final_df['price'] + final_df['freight_value']
print(final_df['total_price'])
#drop price and freight_value in final_df
final_df = final_df.drop(columns=['price', 'freight_value'])


#make the review score to be binary 1-3 as poor reviews and 4-5 as good reviews
final_df['reviews'] = (final_df['review_score'] > 3).astype(int)
print(final_df['reviews'])


#get the average scores for each categories
category_avg_scores = final_df.groupby('product_category_name_english')['review_score'].mean()
final_df['category_scores'] = final_df['product_category_name_english'].map(category_avg_scores)
print(final_df['category_scores'])



#get the average scores for each producers
sellers_avg_scores = final_df.groupby('seller_id')['review_score'].mean()
final_df['seller_scores'] = final_df['seller_id'].map(sellers_avg_scores)
print(final_df['seller_scores'])



#get the avergae scores by customer state
state_avg_scores = final_df.groupby('customer_state')['review_score'].mean()
final_df['state_scores'] = final_df['customer_state'].map(state_avg_scores)
print(final_df['state_scores'])

# #get the privious avargae score by customers
# customers_avg_scores = final_df.groupby('customer_id')['review_score'].mean()
# final_df['customer_average_scores'] = final_df['customer_id'].map(customers_avg_scores)
# print(final_df['customer_average_scores'])


#print(final_df.head())


columns_to_drop = ['order_delivered_customer_date','customer_state','customer_id','seller_id','order_approved_at','order_estimated_delivery_date','product_category_name_english','review_score']  # drop unwanted columns
final_df = final_df.drop(columns=columns_to_drop)


print(final_df.head())


# #minimize the categories

# # Extract the main category, handling cases where there is no underscore
# def extract_main_category(text):
#     if '_' in text:
#         return text.split('_')[0]
#     else:
#         return text  # Return the full text if there's no underscore

# df['main_category'] = final_df['product_category_name_english'].apply(extract_main_category)

# df.to_csv('merged_data.csv', index=False)

# print(df['main_category'])

"""# Train datasets"""

y_value = final_df['reviews'] # set the y
y_values = np.ravel(y_value) # change to an array (list)

x_values = final_df.drop('reviews', axis=1) # drop the y from the dataframe



# split data into training and test
from sklearn.model_selection  import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(x_values, y_value, test_size = 0.2, random_state=4567, stratify=y_value)

# print the shapes to check everything is OK
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

"""# Modeling"""

#from sklearn.linear_model import LogisticRegression as LogR
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.ensemble import GradientBoostingClassifier as GBDT
from xgboost import XGBClassifier as XGB
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import ConfusionMatrixDisplay as CM
import matplotlib.pyplot as plt

#LogR_algo = LogR()
#LogR_model = LogR_algo.fit(X_train, Y_train)

RF_algo = RF()
RF_model = RF_algo.fit(X_train, Y_train)

GBDT_algo = GBDT()
GBDT_model = GBDT_algo.fit(X_train, Y_train)

XGB_algo = XGB()
XGB_model = XGB_algo.fit(X_train, Y_train)

models = [RF_model, GBDT_model, XGB_model]
names = [ 'Random Forest', 'GBDT', 'XGBDT']

for i in range(3):
  print(f"Model: {names[i]}")

  # predict based on training data
  predict = models[i].predict(X_train)


  cm = CM.from_predictions(Y_train, predict)
  print("Confusion Matrix:")
  print(cm.confusion_matrix)

  # Calculate precision, recall, and F1-score
  precision, recall, f1_score, _ = precision_recall_fscore_support(Y_train, predict, average='macro')
  print(f"Macro Precision: {precision}")
  print(f"Macro Recall: {recall}")
  print(f"Macro F1-score: {f1_score}")
  print("\n")

  # Extract feature importances and plot
  feature_importances = models[i].feature_importances_
  feature_names = X_train.columns  # Use feature names from the DataFrame

  # Sort by importance
  sorted_idx = np.argsort(feature_importances)
  sorted_importances = feature_importances[sorted_idx]
  sorted_features = feature_names[sorted_idx]

  # Plot the Gini importance
  plt.figure(figsize=(10, 6))
  plt.barh(sorted_features, sorted_importances, align="center")
  plt.title(f"Gini Importance: {names[i]}")
  plt.xlabel("Gini Importance")
  plt.ylabel("Features")
  plt.show()

# To check if the model is overfitted using test dataset
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.ensemble import GradientBoostingClassifier as GBDT
from xgboost import XGBClassifier as XGB
from sklearn.metrics import precision_recall_fscore_support



RF_algo = RF()
RF_model = RF_algo.fit(X_test, Y_test)

GBDT_algo = GBDT()
GBDT_model = GBDT_algo.fit(X_test, Y_test)

XGB_algo = XGB()
XGB_model = XGB_algo.fit(X_test, Y_test)

models = [RF_model, GBDT_model, XGB_model]
names = [ 'Random Forest', 'GBDT', 'XGBDT']

for i in range(3):
  print(f"Model: {names[i]}")

  # predict based on training data
  predict = models[i].predict(X_test)


  # Calculate precision, recall, and F1-score
  precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, predict, average='macro')
  print(f"Macro Precision: {precision}")
  print(f"Macro Recall: {recall}")
  print(f"Macro F1-score: {f1_score}")
  print("\n")